{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdoGCpAyWesnq7Eyi5IdES"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "ftOUI4t93NfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install pytest-playwright\n",
        "!playwright install\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "hv_XcWsr3MeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter out the HTML source code (only for SSR pages)."
      ],
      "metadata": {
        "id": "KRPKrwlclYZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup, Comment\n",
        "import requests\n",
        "\n",
        "url = \"https://2717recovery.com/products/recovery-cream\"\n",
        "# url = \"https://bhumi.com.au/products/sateen-sheet-set-stone?variant=46357555839133\"\n",
        "# url = \"https://lyfefuel.com/products/essentials-nutrition-shake\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for tag in soup(['script', 'style', 'meta', 'link']):\n",
        "            tag.decompose()\n",
        "\n",
        "        # remove all comments\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        cleaned_html = str(soup)\n",
        "\n",
        "        # print(cleaned_html)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "veyafOhi_mc0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uses Gemini flash model to determine the classname."
      ],
      "metadata": {
        "id": "RVnuBjjYlp8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv('API_KEY')\n",
        "\n",
        "response = requests.post(\n",
        "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "    headers={\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    },\n",
        "    data=json.dumps({\n",
        "        \"model\": \"google/gemini-2.0-flash-exp:free\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                # \"content\": \"extract the user reviews from this codebase. Return in a json format with review text, review author and stars, if not present then return empty json like {} and nothing else. ALso don't add any code formatatting in output. Here is the code: \" + cleaned_html\n",
        "                \"content\": \"extract the class name of pagination next page button of review section from this codebase. Just return a single word with the classname, if not present then return 'null' and nothing else. Here is the code: \" + cleaned_html\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "\n",
        "    # print(data)\n",
        "    message_content = data['choices'][0]['message']['content']\n",
        "    print(message_content)\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzkDT-afFi7l",
        "outputId": "b2c0a364-28ef-48f0-8176-0e2acf3b4617"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jdgm-paginate__next-page\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An attempt to filter out HTML for pages with CSR. PS: Failed attempt (unknown bug), filter works as expected but for some reason doesn't give desired output when used with LLM"
      ],
      "metadata": {
        "id": "4Zp_NG9nl4sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# URL of the webpage to scrape\n",
        "# url = \"https://bhumi.com.au/products/sateen-sheet-set-stone?variant=46357555839133\"\n",
        "url = \"https://2717recovery.com/products/recovery-cream\"\n",
        "\n",
        "cleaned_html = \"\"\n",
        "rendered_html = \"\"\n",
        "\n",
        "async def scrape_page(url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        await page.goto(url)\n",
        "\n",
        "        rendered_html = await page.content()\n",
        "\n",
        "        soup = BeautifulSoup(rendered_html, 'html.parser')\n",
        "\n",
        "        for tag in soup(['script', 'style', 'meta', 'link']):\n",
        "            tag.decompose()\n",
        "\n",
        "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "            comment.extract()\n",
        "\n",
        "        cleaned_html = str(soup)\n",
        "\n",
        "        # print(cleaned_html)\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "asyncio.run(scrape_page(url))\n"
      ],
      "metadata": {
        "id": "GLPHt6RNIS-X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q99i2ykReL4o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}